{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from resolve_path import ajuste_path, read_input\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_input(\"BD Acid_fechamento_Ago.csv\")\n",
    "pathInput = \"data/input/\"\n",
    "pathInput = ajuste_path(pathInput)\n",
    "\n",
    "# Filtrar apenas arquivos Excel que contenham \"acid\" no nome e tenham extensão .xlsx ou .xls\n",
    "excel_files = [f for f in os.listdir(pathInput) if (\n",
    "    \"acid\" in f.lower()) and (f.endswith('.xlsx') or f.endswith('.xls'))]\n",
    "\n",
    "# Pegar o caminho completo dos arquivos\n",
    "full_paths = [os.path.join(pathInput, f) for f in excel_files]\n",
    "\n",
    "# Verificar se há arquivos que correspondam ao critério\n",
    "if full_paths:\n",
    "    # Identificar o arquivo mais recente\n",
    "    latest_file = max(full_paths, key=os.path.getmtime)\n",
    "    print(f\"Arquivo mais recente com 'acid' no nome: {latest_file}\")\n",
    "else:\n",
    "    print(\"Nenhum arquivo com 'acid' no nome foi encontrado.\")\n",
    "\n",
    "# Identificar o arquivo mais recente\n",
    "latest_file = max(full_paths, key=os.path.getmtime)\n",
    "\n",
    "latest_file_csv = latest_file.replace(\".xlsx\", \".csv\")\n",
    "\n",
    "df = pd.read_csv(latest_file_csv, sep=\"#\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento das colunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "df.columns = df.columns.map(lambda x: unidecode(str(x)))\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formata_elemento(elemento):\n",
    "    # # print(elemento)\n",
    "    if isinstance(elemento, str):\n",
    "        elemento = elemento.lower()\n",
    "        elemento = unidecode(elemento)\n",
    "        elemento = elemento.replace(\"\\n\", \" \")\n",
    "        elemento = elemento.replace(\"\\r\", \" \")\n",
    "        elemento = elemento.replace(\"\\t\", \" \")\n",
    "\n",
    "    return elemento\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(formata_elemento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_dict = {\n",
    "    \"na\": np.nan,\n",
    "    \"nan\": np.nan,\n",
    "    \"n.a\": np.nan,\n",
    "    \"n.a.\": np.nan,\n",
    "    \"n/a\": np.nan,\n",
    "    \"-\": np.nan,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empregado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"empregado\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"classificacao\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"empresa\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fornecedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fornecedor\"] = df[\"fornecedor\"].replace(nan_dict)\n",
    "df[\"fornecedor\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocando próprios com fornecedor como terceiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"fornecedor\"].notna(), \"empregado\"] = \"terceiro\"\n",
    "print(df[\"empregado\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df[\"id\"].str.lstrip(\"\\t\")\n",
    "\n",
    "df[\"id\"] = df[\"id\"].replace(nan_dict)\n",
    "\n",
    "indices_com_repeticao = []\n",
    "print(\"Número de linhas antes da separação de ids agrupados: \", len(df))\n",
    "for index, row in df.iterrows():\n",
    "    if isinstance(row['id'], str) and ' e ' in row['id']:\n",
    "\n",
    "        matriculas = row['id'].replace(' e ', '|')\n",
    "        matriculas = matriculas.replace(', ', '|')\n",
    "        matriculas = matriculas.strip()\n",
    "\n",
    "        nomes = row['nome'].replace(' e ', '|')\n",
    "        nomes = nomes.replace(', ', '|')\n",
    "\n",
    "        lista_matriculas = matriculas.split('|')\n",
    "        lista_nomes = nomes.split('|')\n",
    "\n",
    "        for i in range(len(lista_matriculas)):\n",
    "            nova_linha = row.copy()\n",
    "            nova_linha['id'] = lista_matriculas[i]\n",
    "            nova_linha['nome'] = lista_nomes[i]\n",
    "            df = pd.concat([df, pd.DataFrame([nova_linha])], ignore_index=True)\n",
    "\n",
    "        indices_com_repeticao.append(index)\n",
    "\n",
    "print(\"Número de linhas após a separação de ids agrupados: \", len(df))\n",
    "df = df.drop(indices_com_repeticao)\n",
    "print(\"Número de linhas após a remoção de ids agrupados: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dia\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mes\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ano\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a coluna data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preenchendo com 12 horas o que está nan para não perder a singularidade da hora\n",
    "df[\"hora\"] = df[\"hora\"].fillna(12)\n",
    "\n",
    "valores = df[\"hora\"].unique()\n",
    "\n",
    "subs = {}\n",
    "for hora in valores:\n",
    "    try:\n",
    "        subs[hora] = hora[:2]\n",
    "    except:\n",
    "        # subs['Hora'][np.nan] = 'nao informado'\n",
    "        pass\n",
    "\n",
    "print(json.dumps(subs, indent=4))\n",
    "\n",
    "df[\"hora\"] = df[\"hora\"].replace(\n",
    "    subs)\n",
    "\n",
    "# Criando coluna \"Data\" a partir das colunas 'day', 'month' e 'year'\n",
    "df['data'] = pd.to_datetime(\n",
    "    df[['ano', 'mes', 'dia', 'hora']].rename(\n",
    "        columns={'ano': 'year', 'mes': 'month', 'dia': 'day', 'hora': 'hour'}\n",
    "    ))\n",
    "\n",
    "# Renomeando as colunas de volta\n",
    "df.rename(\n",
    "    columns={'day': 'dia', 'month': 'mes', 'year': 'ano'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"descricao\"] = df[\"descricao\"].replace(nan_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_coordenada(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\",\", \".\")  # Substitui vírgula por ponto\n",
    "        value = value.replace(\"°\", \"\")  # Remove \"°\"\n",
    "        return float(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def corrige_coordenada(df):\n",
    "    # Corrigir longitude digitada errada (número muito grande)\n",
    "    df[\"longitude\"] = df[\"longitude\"].apply(\n",
    "        lambda x: x / 100000 if pd.notna(x) and x < -100 else x\n",
    "    )\n",
    "    # Corrigir sinal das coordenadas trocado\n",
    "    df[\"longitude\"] = df[\"longitude\"].apply(\n",
    "        lambda x: x * (-1) if pd.notna(x) and x > 0 else x\n",
    "    )\n",
    "    # Corrigir vírgulas das coordenadas digitadas errado\n",
    "    df[\"longitude\"] = df[\"longitude\"].apply(\n",
    "        lambda x: x * 10 if pd.notna(x) and x > -10 else x\n",
    "    )\n",
    "    # Desconsiderar pontos na Antártica\n",
    "    df = df[df[\"latitude\"].apply(lambda x: pd.notna(\n",
    "        x) and x > -40 if pd.notna(x) else True)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"latitude\", \"longitude\"]].isna().sum())\n",
    "\n",
    "print(df[\"latitude\"].unique())\n",
    "df[\"latitude\"] = df[\"latitude\"].apply(processa_coordenada)\n",
    "print(df[\"latitude\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"longitude\"].unique())\n",
    "df[\"longitude\"] = df[\"longitude\"].apply(processa_coordenada)\n",
    "print(df[\"longitude\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corrige_coordenada(df)\n",
    "print(df[[\"latitude\", \"longitude\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"potencial\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathUtil = \"data/util/\"\n",
    "pathUtil = ajuste_path(pathUtil)\n",
    "\n",
    "df.to_csv(pathUtil + \"acidentes/\" + \"acidentes_preprocessado.csv\",\n",
    "          sep=\"#\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
